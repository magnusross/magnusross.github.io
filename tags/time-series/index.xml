<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Time series on Magnus Ross</title>
    <link>/tags/time-series/</link>
    <description>Recent content in Time series on Magnus Ross</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/time-series/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Time series benchmarks are broken</title>
      <link>/posts/tsf_benchmarks/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/tsf_benchmarks/</guid>
      <description>Benchmark tasks are a cornerstone of ML research, and a large proportion new research in pretty much every subfield follows the following template:
 Introduce new layer, architecture, optimiser, inference scheme&amp;hellip; Run the model of a set of benchmark tasks Make a big table with your model/method and a load of other similar ones1  Although this paradigm has some issues, it can be a useful way of doing things. It has arguably driven at least some of the progress we have seen in recent years by making it easy for researchers to quickly iterate without worrying too much about applications.</description>
    </item>
    
  </channel>
</rss>
