<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning on Magnus Ross</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine learning on Magnus Ross</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Moms, Models and Medicine</title>
      <link>/posts/moms-models-medicine/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>/posts/moms-models-medicine/</guid>
      <description>Machine learning for health (ML4H) has a problem with deployment in the clinic. Each year, thousands of papers are published describing ML systems for all kinds of medical problems, from antimicrobial resistance to radiology. Unfortunately, only a tiny proportion of these models—estimates put the number at around 1%—are ever prospectively evaluated, meaning deployed and tested in the real world. Systems with no prospective evaluation cannot be used by clinicians and therefore cannot impact patient outcomes.</description>
    </item>
    
    <item>
      <title>Time series benchmarks are broken</title>
      <link>/posts/tsf_benchmarks/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/tsf_benchmarks/</guid>
      <description>Benchmark tasks are a cornerstone of ML research, and a large proportion new research in pretty much every subfield follows the following template:
 Introduce new layer, architecture, optimiser, inference scheme&amp;hellip; Run the model on a set of benchmark tasks Make a big table with your model/method and a load of other similar ones1  Although this paradigm has some issues, it can be a useful way of doing things. It has arguably driven at least some of the progress we have seen in recent years by making it easy for researchers to quickly iterate without worrying too much about applications.</description>
    </item>
    
    <item>
      <title>Lagrangians with PyTorch Part #1</title>
      <link>/posts/l1/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/l1/</guid>
      <description>(Note: This series of posts is closely related to and inspired by this paper from Miles Cranmer, Sam Greydanus, Stephan Hoyer and others. To accompany the paper they also wrote a brilliant blog post about the work which I would encourage you to read)
I really enjoy Lagrangian mechanics, in fact, I would go so far as to say that studying it was one of the best parts of my physics degree.</description>
    </item>
    
  </channel>
</rss>
