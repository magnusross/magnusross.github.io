<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Maths problems :: </title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
4. Mercer theorem measure and RKHS norm.  Source: Gaussian Processes for Machine Learning by Rasmussen and Williams, Ex. 6.7.1
Date: 2023-04-12
 This problem from the GPML book relates to the effect of the choice of measure when using Mercer&amp;rsquo;s theorem to compute kernel eigenfunctions on the resulting norm in the RKHS induced by that kernel." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="/problems/" />




<link rel="stylesheet" href="/assets/style.css">

  <link rel="stylesheet" href="/assets/blue.css">






<link rel="apple-touch-icon" href="/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="/img/favicon/blue.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Maths problems">
<meta property="og:description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
4. Mercer theorem measure and RKHS norm.  Source: Gaussian Processes for Machine Learning by Rasmussen and Williams, Ex. 6.7.1
Date: 2023-04-12
 This problem from the GPML book relates to the effect of the choice of measure when using Mercer&amp;rsquo;s theorem to compute kernel eigenfunctions on the resulting norm in the RKHS induced by that kernel." />
<meta property="og:url" content="/problems/" />
<meta property="og:site_name" content="" />

  
    <meta property="og:image" content="/img/favicon/blue.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">













</head>
<body class="blue">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/about">
  <div class="logo">
    Magnus Ross
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/posts">Posts</a></li>
        
      
        
          <li><a href="/problems">Problems</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/posts">Posts</a></li>
      
    
      
        <li><a href="/problems">Problems</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="/problems/">Maths problems</a></h1>
  <div class="post-meta">
    
    
  </div>

  

  

  

  <div class="post-content"><div>
        <p>This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.</p>
<h1 id="4-mercer-theorem-measure-and-rkhs-norm">4. Mercer theorem measure and RKHS norm.<a href="#4-mercer-theorem-measure-and-rkhs-norm" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: Gaussian Processes for Machine Learning by Rasmussen and Williams, Ex. 6.7.1</p>
<p>Date: 2023-04-12</p>
<hr>
<p>This problem from the GPML book relates to the effect of the choice of measure when using Mercer&rsquo;s theorem to compute kernel eigenfunctions on the resulting norm in the RKHS induced by that kernel. In the problem we show that in the finite dimensional case (this also applies in the $\infty$ dimensions case but it is then harder to show), the RKHS norm is independent of the measure chosen. This is interesting to me because when first learning about Mercer&rsquo;s theorem, I was confused by how the measure the eigenfunctions are computed w.r.t is chosen when using the theorem in practice. This question shows that in for some important applications, the measure doesn&rsquo;t matter.</p>
<p><strong>Problem</strong></p>
<p>We motivate the fact that the RKHS norm does not depend on the density $p(x)$ using a finite-dimensional analogue. Consider the n-dimensional vector $\mathbf f$, and let the $n \times n$ matrix $\Phi$ be comprised of non-colinear columns $\phi_1,\dots, \phi_n$. Then $\mathbf f$ can be expressed as a linear combination of these basis vectors $\mathbf f = \sum^n_{i=1} c_i\phi_i = \Phi \mathbf c$ for some coefficients $\{c_i\}$. Let the $\phi$s be eigenvectors of the covariance matrix $K$ w.r.t. a diagonal matrix $P$ with non-negative entries, so that $KP \Phi = \Phi\Lambda$, where $\Lambda$ is a diagonal matrix containing the eigenvalues. Note that $\Phi^\top P\Phi = I$. Show that $\sum^n_{i=1}c^2_i/\lambda_i=\mathbf c^\top\Lambda^{-1}\mathbf c=\mathbf f^\top K^{-1} \mathbf f$, and thus observe that $\mathbf f^\top K^{-1} \mathbf f$ can be expressed as  $ \mathbf c^\top\Lambda\mathbf c$ for any valid $P$ and corresponding $\Phi$. (Note the matrix $P$ here represents the measure.)</p>
<p><strong>Solution</strong></p>
<p>This solution is a bit strange as I got stuck and this is the only way I could think to do it, I am sure there is a better way. First using that $\Phi^\top P\Phi = I$ we can see that,
$$
KP\Phi=\Phi\Lambda \implies \Phi^\top PKP\Phi=\Lambda.
$$
We can then write
\begin{align}
\mathbf c^\top \Lambda^{-1} \mathbf c &amp;= \mathbf c^\top (\Phi^\top PKP\Phi)^{-1} \mathbf c \\\<br>
&amp;= \mathbf c^\top \Phi^{-1}P^{-1}K^{-1}P^{-1}\Phi^{-\top} \mathbf c \\\<br>
&amp;= \mathbf c^\top \Phi^\top P P^{-1}K^{-1}P^{-1} P \Phi \mathbf c \\\<br>
&amp;=\mathbf f^{\top} K^{-1} \mathbf f
\end{align}
as required, where on the second to last line we have twice used the fact that $\Phi^{-1}=\Phi^\top P$.</p>
<h1 id="3--converting-state-misalignment-in-probabilistic-numeric-ode-solver-to-sde">3.  Converting state misalignment in probabilistic numeric ODE solver to SDE.<a href="#3--converting-state-misalignment-in-probabilistic-numeric-ode-solver-to-sde" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 38.1</p>
<p>Date: 2023-04-03</p>
<hr>
<p>It is only the third week and I have already failed to manage my goal of posting one solution per week, not a good sign for the future&hellip; In my defense, I was busy last week attending the <a href="https://www.probnumschool.org/pages/home.html">Probabilistic Numerics Spring School</a>, at which I learned lots about probabilistic ODE solvers, on which this weeks late question is based.</p>
<p>Probabilistic ODE solvers work by placing a Markovian GP prior (usually the $q$-times integrated Wiener process) over the solution, given this prior, we then condition on the fact samples of this GP must solve the ODE at a given set of time steps, that is to say the derivatives of the GP prior must match the derivative function of the ODE. We can infer the solution to the ODE by using filtering and smoothing techniques from signal processing. In this question, the task is to form an SDE for the state misalignment, i.e the difference between the derivatives of the prior and the ODE derivative function. It is interesting to note that we can also condition on other properties of the system, for example, conditioning on energy conservation in a Hamiltonian system leads to a symplectic solver, see <a href="https://proceedings.mlr.press/v151/bosch22a.html">here</a> for details. The whole idea is fascinating and deserves a look if you have never come across it.</p>
<p><strong>Problem</strong></p>
<p>We take our prior to be the SDE
$$
dX(t)=FX(t)dt + Ld\omega,
$$
with $X \in\mathbb{R}^{D}$, $F,L\in\mathbb{R}^{D \times D}$ and $d\omega \in\mathbb{R}^{D}$ being an increment of the Wiener process with diffusion $Q\in\mathbb{R}^{D \times D}$. We also have two projection matrices which allow us to obtain the parts of the SDE prior that represent ODE state, and ODE state derivative,
$$
\mathbf x(t) = H_0X(t), \qquad \mathbf x'(t) = HX(t).
$$
with $H,H_0\in\mathbb{R}^{d \times D}$ where $d$ is our ODE dimension. The state misalignment is given by
$$
Z(t) = \phi(X(t))=HX(t) - f(H_0X(t)).
$$
where $f: \mathbb{R}^D \mapsto \mathbb{R}^D$ is our ODE derivative function and $Z(t)\in\mathbb{R}^d$ is called the observation process, and is given by another SDE. Our task is to find an expression for this SDE and state under which conditions it is Gaussian process.</p>
<p><strong>Solution</strong></p>
<p>The Itô formula for transformations of SDEs by scalar funtion $g(X(t))$ is given by
$$
dg = (\nabla g) dX + \frac{1}{2}\operatorname{tr}[(\nabla^\top \nabla g) dXdX^\top].
$$</p>
<p>(Note: here I have used the gradient as row vector) We need to use this to compute the the transformation represented by the state misalignment, which is a vector function. Fortunately, the Itô formula applies to each componnt, so
$$
d\phi_i = (\nabla \phi_i) dX + \frac{1}{2}\operatorname{tr}[(\nabla^\top \nabla \phi_i) dXdX^\top].
$$
with
$$
\phi_i = \sum_j H_{i,j} X_j + f_i(H_0 X).
$$
We now need to compute the Jacobian and the Hessian given in the formula. This took me a long time as my vector calculus is a little rusty, and I am not 100% sure I have the correct answer but I got them to be,
$$
\nabla \phi_i = H_{i,-} + \nabla f_i(H_0 X)H_0^\top
$$
where $\nabla f_i$ is the Jacobian of $f$ for output $i$ and $H_{i,-}\in \mathbb{R}^D$ is the $i$-th column of $H$, and for the Hessain,</p>
<p>$$
\nabla^\top\nabla \phi_i = H_0[\nabla^\top\nabla f_i(H_0 X)]H_0^\top
$$
where $\nabla^\top\nabla f_i$ is the Hessian of $f$ for output $i$. We also need that
\begin{align}
dXdX^\top &amp;= (FX(t)dt + Ld\omega)(FX(t)dt + Ld\omega)^\top \\\<br>
&amp;=LQL^\top dt
\end{align}
where we use the combination rules for mixed differentials given in the <a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma">Itô formula</a>. Subbing all this into the formula and collecting terms, we obtain the SDEs,
\begin{align}
d\phi_i = &amp;\left(H_{i,-}FX(t) + \nabla f_i(H_0 X)H_0^\top X(t) + \frac{1}{2}\operatorname{tr}(H_0[\nabla^\top\nabla f_i(H_0 X)]H_0^\top LQL^\top)\right)dt \\\<br>
&amp;+ (H_{i,-}+ \nabla f_i(H_0 X)H_0^\top)Ld\omega
\end{align}
which can then be simulated to give the observation process. This SDE will be a GP when $f$ is linear, since GP representations of SDEs do not have multiplicative noise (see <a href="https://users.aalto.fi/~asolin/sde-book/sde-book.pdf">Särkkä and Solin book</a>, 6.1), so we require that the gradient of $f$ be independent of $X$ which is only true when we have a linear derivative function.</p>
<h1 id="2--relationship-between-the-frobenius-norm-and-singular-values-in-svd">2.  Relationship between the Frobenius norm and singular values in SVD.<a href="#2--relationship-between-the-frobenius-norm-and-singular-values-in-svd" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: ChatGPT, Mathematics for Machine Learning by Deisenroth, Faisal and Ong</p>
<p>Date: 2023-03-22</p>
<hr>
<p><strong>Problem</strong></p>
<p>Note: I was brushing up on my SVD using the brilliant &ldquo;Mathematics for Machine Learning&rdquo; book, but the exercises listed in the book for SVD were a bit basic, so I decided to try use ChatGPT to generate a question. The problem below is what came out, quite impressive, although there were quite a few errors in the question that I had to fix (e.g the dimensionalities of the matrices).</p>
<p>Let $A$ be an $m \times n$ with $m\geq n$ matrix with rank $r$, and let its singular value decomposition (SVD) be given by $A = U \Sigma V^T$, where $U$ is an $m \times m$ orthonormal matrix, $\Sigma$ is an $m \times n$ diagonal matrix with non-negative entries $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n &gt; 0$, and $V$ is an $n \times n$ orthonormal matrix. Show that the Frobenius norm of $A$ is equal to the square root of the sum of the squares of the singular values.</p>
<p><strong>Solution</strong></p>
<p>The Frobenius norm is given by,</p>
<p>$$
||A||^2_F = \sum_{i,j}A^2_{i,j}
$$
which we can equivalently write (it is not so hard to show this) as
$$
||A||^2_F = \operatorname{tr}(A^\top A).
$$
Subbing in the SVD representation of $A$, we get
\begin{align}
\operatorname{tr}(A^\top A) &amp;= \operatorname{tr}\left( (U\Sigma V^\top)^\top(U\Sigma V^\top)\right)\\\<br>
&amp;= \operatorname{tr}\left(V\Sigma^\top U^\top U\Sigma V^\top\right) \\\<br>
&amp;= \operatorname{tr}\left(V\Sigma^\top \Sigma V^\top\right).
\end{align}
We can see by inspection that $\Sigma^\top\Sigma = \operatorname{diag}(\sigma^2_1,  \dots, \sigma^2_n)=\Lambda$. This implies that
$$
[V\Lambda V^\top]_{ii} = \sigma_i^2\mathbf{v}^\top_i\mathbf{v}_i = \sigma_i^2,
$$
where $\mathbf{v}_i$ is column $i$ of $V$ and the second equality follows from the othonormality of $V$. Putting it all together we get that
$$
||A||^2_F = \operatorname{tr}(A^\top A) = \sum_i \sigma_i^2,
$$
as required.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<hr>
<h1 id="1--winners-curse-in-bayesian-optimisation">1.  Winners curse in Bayesian optimisation.<a href="#1--winners-curse-in-bayesian-optimisation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 32.2</p>
<p>Date: 2023-03-14</p>
<hr>
<p><strong>Problem</strong></p>
<p>Take an objective function $f(x)$ with GP prior and i.i.d. Gaussian noise with variance $\sigma^2$, so $y_i = f(x_i) + \epsilon_i$ with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$. Assume two locations $x_1, x_2$ are sufficiently separated that their function values $f(x_1)$ and $f(x_2)$ are independent. Compute the posterior $p(\epsilon_1-\epsilon_2|y_1, y_2)$ and use it to justify that the expected improvement in Bayesian optimisation suffers winners curse for a noisy objective. The winners curse says that for noisy optimisation problems, the lowest value is also likely the most noisy.</p>
<p><strong>Solution</strong></p>
<p>First we need to compute $p(\epsilon_i| y_i)$, which we can do using Bayes rule,
$$
p(\epsilon_i| y_i) = \frac{p(y_i| \epsilon_i)p(\epsilon_i)}{p(y_i)}
$$
we have that $p(\epsilon_i) = \mathcal{N}(\epsilon_i; 0, \sigma^2)$, and that $p(y_i| \epsilon_i) =  \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)$ where we have assumed that the prior over $f$ has 0 mean, and variance $\sigma_f^2$. So we have,
$$
p(\epsilon_i| y_i) \propto \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)\mathcal{N}(\epsilon_i; 0, \sigma^2)
$$
which we can identify as a product of Gaussian PDFs. This results in another Gaussian PDF, see e.g <a href="http://www.lucamartino.altervista.org/2003-003.pdf">these notes</a>,
$$
p(\epsilon_i| y_i) = \mathcal{N}(\epsilon_i; \mu_{\epsilon_i}, \sigma^2_{\epsilon_i}),
$$
where
\begin{align}
\mu_{\epsilon_i} &amp;= \frac{y_i\sigma^2}{\sigma^2 + \sigma_f^2}, \\\<br>
\sigma^2_{\epsilon_i} &amp;= \frac{\sigma_f^2\sigma^2}{\sigma^2 + \sigma_f^2}.
\end{align}
We can see here that the variance is independent of data. Now we have computed the posterior of the noise given the data, we can compute the posterior of the difference between the noises given the data. Since the function values at the input points are independent then the posterior over the difference is just the difference of independent Gaussian random variables, in which case we simply subtract the means and sum the variances, to give
$$
p(\epsilon_1 - \epsilon_2=\Delta| y_1, y_2) = \mathcal{N}\left(\Delta; \frac{\sigma^2}{\sigma^2 + \sigma_f^2}(y_1-y_2), 2\sigma^2_{\epsilon}\right).
$$
The key point here is that the mean of this distribution depends on the difference between the data values. So
$$
y_1&gt;y_2 \implies \mathbb{E}[\epsilon_1 - \epsilon_2]&gt; 0 \implies \mathbb{E}[\epsilon_1] &gt; \mathbb{E}[\epsilon_2].
$$
If we assume that the values are lower that the prior mean (i.e. 0), which is to be expected, then we expect the noise for the lower point to be a larger (absolute) value. This is the winners curse described in the book.</p>

      </div></div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2023 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>


<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>







  
</div>

</body>
</html>
