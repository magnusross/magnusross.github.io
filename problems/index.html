<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Maths problems :: </title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
1. Winners curse in Bayesian optimisation.  Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 32.2
Date: 2023-03-14
 Problem
Take an objective function $f(x)$ with GP prior and i." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="/problems/" />




<link rel="stylesheet" href="/assets/style.css">

  <link rel="stylesheet" href="/assets/blue.css">






<link rel="apple-touch-icon" href="/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="/img/favicon/blue.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Maths problems">
<meta property="og:description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
1. Winners curse in Bayesian optimisation.  Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 32.2
Date: 2023-03-14
 Problem
Take an objective function $f(x)$ with GP prior and i." />
<meta property="og:url" content="/problems/" />
<meta property="og:site_name" content="" />

  
    <meta property="og:image" content="/img/favicon/blue.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">













</head>
<body class="blue">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/about">
  <div class="logo">
    Magnus Ross
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/posts">Posts</a></li>
        
      
        
          <li><a href="/problems">Problems</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/posts">Posts</a></li>
      
    
      
        <li><a href="/problems">Problems</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="/problems/">Maths problems</a></h1>
  <div class="post-meta">
    
    
  </div>

  

  

  

  <div class="post-content"><div>
        <p>This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.</p>
<h1 id="1--winners-curse-in-bayesian-optimisation">1.  Winners curse in Bayesian optimisation.<a href="#1--winners-curse-in-bayesian-optimisation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 32.2</p>
<p>Date: 2023-03-14</p>
<hr>
<p><strong>Problem</strong></p>
<p>Take an objective function $f(x)$ with GP prior and i.i.d. Gaussian noise with variance $\sigma^2$, so $y_i = f(x_i) + \epsilon_i$ with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$. Assume two locations $x_1, x_2$ are sufficiently separated that their function values $f(x_1)$ and $f(x_2)$ are independent. Compute the posterior $p(\epsilon_1-\epsilon_2|y_1, y_2)$ and use it to justify that the expected improvement in Bayesian optimisation suffers winners curse for a noisy objective. The winners curse says that for noisy optimisation problems, the lowest value is also likely the most noisy.</p>
<p><strong>Solution</strong></p>
<p>First we need to compute $p(\epsilon_i| y_i)$, which we can do using Bayes rule,
$$
p(\epsilon_i| y_i) = \frac{p(y_i| \epsilon_i)p(\epsilon_i)}{p(y_i)}
$$
we have that $p(\epsilon_i) = \mathcal{N}(\epsilon_i; 0, \sigma^2)$, and that $p(y_i| \epsilon_i) =  \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)$ where we have assumed that the prior over $f$ has 0 mean, and variance $\sigma_f^2$. So we have,
$$
p(\epsilon_i| y_i) \propto \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)\mathcal{N}(\epsilon_i; 0, \sigma^2)
$$
which we can identify as a product of Gaussian PDFs. This results in another Gaussian PDF, see e.g <a href="http://www.lucamartino.altervista.org/2003-003.pdf">these notes</a>,
$$
p(\epsilon_i| y_i) = \mathcal{N}(\epsilon_i; \mu_{\epsilon_i}, \sigma^2_{\epsilon_i}),
$$
where
\begin{align}
\mu_{\epsilon_i} &amp;= \frac{y_i\sigma^2}{\sigma^2 + \sigma_f^2}, \\\<br>
\sigma^2_{\epsilon_i} &amp;= \frac{\sigma_f^2\sigma^2}{\sigma^2 + \sigma_f^2}.
\end{align}
We can see here that the variance is independent of data. Now we have computed the posterior of the noise given the data, we can compute the posterior of the difference between the noises given the data. Since the function values at the input points are independent then the posterior over the difference is just the difference of independent Gaussian random variables, in which case we simply subtract the means and sum the variances, to give
$$
p(\epsilon_1 - \epsilon_2=\Delta| y_1, y_2) = \mathcal{N}\left(\Delta; \frac{\sigma^2}{\sigma^2 + \sigma_f^2}(y_1-y_2), 2\sigma^2_{\epsilon}\right).
$$
The key point here is that the mean of this distribution depends on the difference between the data values. So
$$
y_1&gt;y_2 \implies \mathbb{E}[\epsilon_1 - \epsilon_2]&gt; 0 \implies \mathbb{E}[\epsilon_1] &gt; \mathbb{E}[\epsilon_2].
$$
If we assume that the values are lower that the prior mean (i.e. 0), which is to be expected, then we expect the noise for the lower point to be a larger (absolute) value. This is the winners curse described in the book.</p>

      </div></div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>Â© 2023 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>


<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>







  
</div>

</body>
</html>
