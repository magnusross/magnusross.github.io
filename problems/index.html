<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Maths problems :: </title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
2. Relationship between the Frobenius norm and singular values in SVD.  Source: ChatGPT, Mathematics for Machine Learning by Deisenroth, Faisal and Ong" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="/problems/" />




<link rel="stylesheet" href="/assets/style.css">

  <link rel="stylesheet" href="/assets/blue.css">






<link rel="apple-touch-icon" href="/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="/img/favicon/blue.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Maths problems">
<meta property="og:description" content="This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
2. Relationship between the Frobenius norm and singular values in SVD.  Source: ChatGPT, Mathematics for Machine Learning by Deisenroth, Faisal and Ong" />
<meta property="og:url" content="/problems/" />
<meta property="og:site_name" content="" />

  
    <meta property="og:image" content="/img/favicon/blue.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">













</head>
<body class="blue">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/about">
  <div class="logo">
    Magnus Ross
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/posts">Posts</a></li>
        
      
        
          <li><a href="/problems">Problems</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/posts">Posts</a></li>
      
    
      
        <li><a href="/problems">Problems</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="/problems/">Maths problems</a></h1>
  <div class="post-meta">
    
    
  </div>

  

  

  

  <div class="post-content"><div>
        <p>This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.</p>
<h1 id="2--relationship-between-the-frobenius-norm-and-singular-values-in-svd">2.  Relationship between the Frobenius norm and singular values in SVD.<a href="#2--relationship-between-the-frobenius-norm-and-singular-values-in-svd" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: ChatGPT, Mathematics for Machine Learning by Deisenroth, Faisal and Ong</p>
<p>Date: 2023-03-22</p>
<hr>
<p><strong>Problem</strong></p>
<p>Note: I was brushing up on my SVD using the brilliant &ldquo;Mathematics for Machine Learning&rdquo; book, but the exercises listed in the book for SVD were a bit basic, so I decided to try use ChatGPT to generate a question. The problem below is what came out, quite impressive, although there were quite a few errors in the question that I had to fix (e.g the dimensionalities of the matrices).</p>
<p>Let $A$ be an $m \times n$ with $m\geq n$ matrix with rank $r$, and let its singular value decomposition (SVD) be given by $A = U \Sigma V^T$, where $U$ is an $m \times m$ orthonormal matrix, $\Sigma$ is an $m \times n$ diagonal matrix with non-negative entries $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n &gt; 0$, and $V$ is an $n \times n$ orthonormal matrix. Show that the Frobenius norm of $A$ is equal to the square root of the sum of the squares of the singular values.</p>
<p><strong>Solution</strong></p>
<p>The Frobenius norm is given by,</p>
<p>$$
||A||^2_F = \sum_{i,j}A^2_{i,j}
$$
which we can equivalently write (it is not so hard to show this) as
$$
||A||^2_F = \operatorname{tr}(A^\top A).
$$
Subbing in the SVD representation of $A$, we get
\begin{align}
\operatorname{tr}(A^\top A) &amp;= \operatorname{tr}\left( (U\Sigma V^\top)^\top(U\Sigma V^\top)\right)\\\<br>
&amp;= \operatorname{tr}\left(V\Sigma^\top U^\top U\Sigma V^\top\right) \\\<br>
&amp;= \operatorname{tr}\left(V\Sigma^\top \Sigma V^\top\right).
\end{align}
We can see by inspection that $\Sigma^\top\Sigma = \operatorname{diag}(\sigma^2_1,  \dots, \sigma^2_n)=\Lambda$. This implies that
$$
[V\Lambda V^\top]_{ii} = \sigma_i^2\mathbf{v}^\top_i\mathbf{v}_i = \sigma_i^2,
$$
where $\mathbf{v}_i$ is column $i$ of $V$ and the second equality follows from the othonormality of $V$. Putting it all together we get that
$$
||A||^2_F = \operatorname{tr}(A^\top A) = \sum_i \sigma_i^2,
$$
as required.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<hr>
<h1 id="1--winners-curse-in-bayesian-optimisation">1.  Winners curse in Bayesian optimisation.<a href="#1--winners-curse-in-bayesian-optimisation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<hr>
<p>Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 32.2</p>
<p>Date: 2023-03-14</p>
<hr>
<p><strong>Problem</strong></p>
<p>Take an objective function $f(x)$ with GP prior and i.i.d. Gaussian noise with variance $\sigma^2$, so $y_i = f(x_i) + \epsilon_i$ with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$. Assume two locations $x_1, x_2$ are sufficiently separated that their function values $f(x_1)$ and $f(x_2)$ are independent. Compute the posterior $p(\epsilon_1-\epsilon_2|y_1, y_2)$ and use it to justify that the expected improvement in Bayesian optimisation suffers winners curse for a noisy objective. The winners curse says that for noisy optimisation problems, the lowest value is also likely the most noisy.</p>
<p><strong>Solution</strong></p>
<p>First we need to compute $p(\epsilon_i| y_i)$, which we can do using Bayes rule,
$$
p(\epsilon_i| y_i) = \frac{p(y_i| \epsilon_i)p(\epsilon_i)}{p(y_i)}
$$
we have that $p(\epsilon_i) = \mathcal{N}(\epsilon_i; 0, \sigma^2)$, and that $p(y_i| \epsilon_i) =  \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)$ where we have assumed that the prior over $f$ has 0 mean, and variance $\sigma_f^2$. So we have,
$$
p(\epsilon_i| y_i) \propto \mathcal{N}(y_i; \epsilon_i, \sigma_f^2)\mathcal{N}(\epsilon_i; 0, \sigma^2)
$$
which we can identify as a product of Gaussian PDFs. This results in another Gaussian PDF, see e.g <a href="http://www.lucamartino.altervista.org/2003-003.pdf">these notes</a>,
$$
p(\epsilon_i| y_i) = \mathcal{N}(\epsilon_i; \mu_{\epsilon_i}, \sigma^2_{\epsilon_i}),
$$
where
\begin{align}
\mu_{\epsilon_i} &amp;= \frac{y_i\sigma^2}{\sigma^2 + \sigma_f^2}, \\\<br>
\sigma^2_{\epsilon_i} &amp;= \frac{\sigma_f^2\sigma^2}{\sigma^2 + \sigma_f^2}.
\end{align}
We can see here that the variance is independent of data. Now we have computed the posterior of the noise given the data, we can compute the posterior of the difference between the noises given the data. Since the function values at the input points are independent then the posterior over the difference is just the difference of independent Gaussian random variables, in which case we simply subtract the means and sum the variances, to give
$$
p(\epsilon_1 - \epsilon_2=\Delta| y_1, y_2) = \mathcal{N}\left(\Delta; \frac{\sigma^2}{\sigma^2 + \sigma_f^2}(y_1-y_2), 2\sigma^2_{\epsilon}\right).
$$
The key point here is that the mean of this distribution depends on the difference between the data values. So
$$
y_1&gt;y_2 \implies \mathbb{E}[\epsilon_1 - \epsilon_2]&gt; 0 \implies \mathbb{E}[\epsilon_1] &gt; \mathbb{E}[\epsilon_2].
$$
If we assume that the values are lower that the prior mean (i.e. 0), which is to be expected, then we expect the noise for the lower point to be a larger (absolute) value. This is the winners curse described in the book.</p>

      </div></div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>Â© 2023 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>


<script src="/assets/main.js"></script>
<script src="/assets/prism.js"></script>







  
</div>

</body>
</html>
