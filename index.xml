<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Simulation is easy, probability is hard...</title>
      <link>/posts/montecarlo/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/montecarlo/</guid>
      <description>There are many interesting things that can be learned about probabilistic modeling from the world of trading and finance, where, perhaps due to the amount of money the line, attitudes to problems are generally very different to those of the typical statistician or ML practitioner. Two books I have enjoyed recently in the area are The Education of a Speculator by Victor Niederhoffer and Fooled by Randomness by Nassim Nicholas Taleb.</description>
    </item>
    
    <item>
      <title>Talk at 3rd Sheffield Workshop on Structural Dynamics</title>
      <link>/posts/talk1/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/talk1/</guid>
      <description>Near the end of last year I had the privilege of being invited at the 3rd Sheffield Workshop on Structural Dynamics. I spoke for 15 minutes about some work I have been doing on the application of Gaussian processes to the non-parametric learning of Volterra kernel functions. The event as a whole was super interesting, and I would fully recommend the other talks.
You can find my talk here.</description>
    </item>
    
    <item>
      <title>Podcasts about ML</title>
      <link>/posts/podcasts/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/podcasts/</guid>
      <description>There are a lot of podcasts out there, and this includes loads that are related to machine learning in some way. Over the summer I&amp;rsquo;ve worked my way through a fair few of them, so I thought I&amp;rsquo;d compile a list of my favorites. There are a couple of lists like this out there already, but most of them are out of date, either they include podcasts that are now inactive, or leave out ones that have started recently.</description>
    </item>
    
    <item>
      <title>Lagrangians with PyTorch Part #1</title>
      <link>/posts/l1/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/l1/</guid>
      <description>(Note: This series of posts is closely related to and inspired by this paper from Miles Cranmer, Sam Greydanus, Stephan Hoyer and others. To accompany the paper they also wrote a brilliant blog post about the work which I would encourage you to read)
I really enjoy Lagrangian mechanics, in fact, I would go so far as to say that studying it was one of the best parts of my physics degree.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hi! I&amp;rsquo;m Magnus, this is my website, where I will hopefully write about things I find interesting. I have just finished doing a Master&amp;rsquo;s degree in physics at the University of Bristol, and have now joined the machine learning group at the University of Sheffield as a PhD student, working with Mauricio Alvarez on Latent Force Models. I am interested in many different areas of machine learning, but in particular I enjoy those that have some connection to physics (fortunately there are many!</description>
    </item>
    
  </channel>
</rss>
