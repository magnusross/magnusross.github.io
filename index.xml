<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>New paper: Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processess</title>
      <link>/posts/paper2/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/paper2/</guid>
      <description>I spent the summer of 2022 visiting Markus Heinonen at Aalto University in Finland. Together we worked on energy conserving GP models, and I am happy to say our paper on the work was recently accepted to TMLR. Check out the paper, the code, or some visualizations.
We place a GP prior over the Hamiltonian and, using a set of inducing points, map function samples through Hamilton&#39;s equations to obtain system derivative samples, to which we apply ODE solver to obtain sample trajectories.</description>
    </item>
    
    <item>
      <title>New paper: Learning Nonparametric Volterra Kernels with Gaussian Processes</title>
      <link>/posts/paper1/</link>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/paper1/</guid>
      <description>Update: This paper was published at NeurIPS 2021 check out the final version or my presentation.
This work, with Mauricio Alvarez and Mike Smith, has been the main focus of the first year of my PhD, check it out on arXiv.
The sampling process for the model described in the paper, which is a non-parametric, and can represent data generated by non-linear operators.  There will be a blog post explaining the key ideas coming soon!</description>
    </item>
    
    <item>
      <title>Simulation is easy, probability is hard...</title>
      <link>/posts/mc/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/mc/</guid>
      <description>There are many interesting things that can be learned about probabilistic modelling from the world of trading and finance, where, perhaps due to the direct connection to very large sums of money, attitudes to problems are generally very different to those of the typical statistician or ML practitioner. Two books I have enjoyed recently in the area are The Education of a Speculator by Victor Niederhoffer and Fooled by Randomness by Nassim Nicholas Taleb.</description>
    </item>
    
    <item>
      <title>Talk at 3rd Sheffield Workshop on Structural Dynamics</title>
      <link>/posts/talk1/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/talk1/</guid>
      <description>Near the end of last year I had the privilege of being invited at the 3rd Sheffield Workshop on Structural Dynamics. I spoke for 15 minutes about some work I have been doing on the application of Gaussian processes to the non-parametric learning of Volterra kernel functions. The event as a whole was super interesting, and I would fully recommend the other talks.
You can find my talk here.</description>
    </item>
    
    <item>
      <title>Podcasts about ML</title>
      <link>/posts/podcasts/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/podcasts/</guid>
      <description>There are a lot of podcasts out there, and this includes loads that are related to machine learning in some way. Over the summer I&amp;rsquo;ve worked my way through a fair few of them, so I thought I&amp;rsquo;d compile a list of my favorites. There are a couple of lists like this out there already, but most of them are out of date, either they include podcasts that are now inactive, or leave out ones that have started recently.</description>
    </item>
    
    <item>
      <title>Lagrangians with PyTorch Part #1</title>
      <link>/posts/l1/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/l1/</guid>
      <description>(Note: This series of posts is closely related to and inspired by this paper from Miles Cranmer, Sam Greydanus, Stephan Hoyer and others. To accompany the paper they also wrote a brilliant blog post about the work which I would encourage you to read)
I really enjoy Lagrangian mechanics, in fact, I would go so far as to say that studying it was one of the best parts of my physics degree.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hi! I&amp;rsquo;m Magnus, this is my website, where I will hopefully write about things I find interesting. I am third year PhD student at the University of Manchester, working with Mauricio Alvarez on integrating physical information into Gaussian process models. I am interested in many different areas of machine learning, but in particular I enjoy those that have some connection to physics (fortunately there are many!).
Outside of work I like to go running, play music, and look after my plants.</description>
    </item>
    
    <item>
      <title>Maths problems</title>
      <link>/problems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/problems/</guid>
      <description>This is a list of maths problems (from textbooks, lecture notes etc.) that I have been looking at and have found interesting to solve. I am trying to post 1 per week starting from 2023-03 in an attempt to motivate myself to keep my maths skills sharp.
3. Converting state misalignment in probabilistic numeric ODE solver to SDE.  Source: Probabilistic Numerics by Henning, Osborne and Kersting, Ex. 38.1
Date: 2023-04-03</description>
    </item>
    
  </channel>
</rss>
