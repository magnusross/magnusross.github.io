<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Magnus Ross</title>
    <link>/</link>
    <description>Recent content in Home on Magnus Ross</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Moms, Models and Medicine</title>
      <link>/posts/moms-models-medicine/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>/posts/moms-models-medicine/</guid>
      <description>Machine learning for health (ML4H) has a problem with deployment in the clinic. Each year, thousands of papers are published describing ML systems for all kinds of medical problems, from antimicrobial resistance to radiology. Unfortunately, only a tiny proportion of these models—estimates put the number at around 1%—are ever prospectively evaluated, meaning deployed and tested in the real world. Systems with no prospective evaluation cannot be used by clinicians and therefore cannot impact patient outcomes.</description>
    </item>
    
    <item>
      <title>Practicing mountain marathon navigation</title>
      <link>/posts/mm_nav_guide/</link>
      <pubDate>Sun, 08 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>/posts/mm_nav_guide/</guid>
      <description>This is a guide that details how to use the Routegadget website to practice route selection for a mountain marathon. While Routegadget&amp;rsquo;s interface is a little old-school, it is a useful and powerful tool for analysing past races. This guide will focus on past races from the SLMM, but this practice is applicable to any mountain marathon.
A first map First, let&amp;rsquo;s look at a past course on Routegadget. Follow this link to view the map and controls for day one of the &amp;ldquo;Harter Fell&amp;rdquo; course of the 2017 edition of the SLMM.</description>
    </item>
    
    <item>
      <title>A Spartan&#39;s guide to trading</title>
      <link>/posts/spartans_guide_to_trading/</link>
      <pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>/posts/spartans_guide_to_trading/</guid>
      <description>Unless you&amp;rsquo;ve been under a rock the past few weeks, you&amp;rsquo;ve probably heard about Elon Musk&amp;rsquo;s gaggle of DOGE lads who have been let loose inside the US Treasury payments system. Since they have apparently been given read/write access to some highly sensitive databases and are likely pushing hastily coded &amp;ldquo;efficiency improvements&amp;rdquo; to main as we speak, it’s in the public interest to understand how good these guys are as engineers.</description>
    </item>
    
    <item>
      <title>Time series benchmarks are broken</title>
      <link>/posts/tsf_benchmarks/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/tsf_benchmarks/</guid>
      <description>Benchmark tasks are a cornerstone of ML research, and a large proportion new research in pretty much every subfield follows the following template:
 Introduce new layer, architecture, optimiser, inference scheme&amp;hellip; Run the model on a set of benchmark tasks Make a big table with your model/method and a load of other similar ones1  Although this paradigm has some issues, it can be a useful way of doing things. It has arguably driven at least some of the progress we have seen in recent years by making it easy for researchers to quickly iterate without worrying too much about applications.</description>
    </item>
    
    <item>
      <title>Expectation maximization for mixture of linear experts</title>
      <link>/posts/em_problem/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/em_problem/</guid>
      <description>The EM algorithm for me is one of those things that I feel I should know back to front, since it&amp;rsquo;s a pretty foundational algorithm in probabilistic ML. Unfortunately though I&amp;rsquo;ve never actually used it explicitly in a model I have built, despite reading about it in various textbooks, so I never properly got to grips with it. If you feel the same way, then hopefully this question will help. It&amp;rsquo;s from the new Murphy book, which is a fantastic reference.</description>
    </item>
    
    <item>
      <title>Distributions of quotients of uniform RVs</title>
      <link>/posts/rvquotients_problem/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/rvquotients_problem/</guid>
      <description>This is a nice problem from the book &amp;ldquo;Cut the knot&amp;rdquo;, which is a compilation of probability riddles and brain teasers. I like the book because many of the problems are about intuitive reasoning, and don&amp;rsquo;t really involve that much technical knowledge of probability theory, so they can be fun to do with people who don&amp;rsquo;t have a formal maths background. Each problem can also generally be solved many different ways, so it&amp;rsquo;s fun to compare solutions.</description>
    </item>
    
    <item>
      <title>Mercer theorem measure and RKHS norm</title>
      <link>/posts/mercerrkhs_problem/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/mercerrkhs_problem/</guid>
      <description>This problem from the GPML book relates to the effect of the choice of measure when using Mercer&amp;rsquo;s theorem to compute kernel eigenfunctions on the resulting norm in the RKHS induced by that kernel. In the problem we show that in the finite dimensional case (this also applies in the ( \infty ) dimensions case but it is then harder to show), the RKHS norm is independent of the measure chosen.</description>
    </item>
    
    <item>
      <title>Converting state misalignment in probabilistic numeric ODE solver to SDE.</title>
      <link>/posts/pn_solver_problem/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/pn_solver_problem/</guid>
      <description>It is only the third week and I have already failed to manage my goal of posting one solution per week, not a good sign for the future&amp;hellip; In my defense, I was busy last week attending the Probabilistic Numerics Spring School, at which I learned lots about probabilistic ODE solvers, on which this weeks late question is based.
Probabilistic ODE solvers work by placing a Markovian GP prior (usually the ( q )-times integrated Wiener process) over the solution, given this prior, we then condition on the fact samples of this GP must solve the ODE at a given set of time steps, that is to say the derivatives of the GP prior must match the derivative function of the ODE.</description>
    </item>
    
    <item>
      <title>Relationship between the Frobenius norm and singular values in SVD</title>
      <link>/posts/frobeniusnorm_problem/</link>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/frobeniusnorm_problem/</guid>
      <description>Problem
Note: I was brushing up on my SVD using the brilliant &amp;ldquo;Mathematics for Machine Learning&amp;rdquo; book, but the exercises listed in the book for SVD were a bit basic, so I decided to try use ChatGPT to generate a question. The problem below is what came out, quite impressive, although there were quite a few errors in the question that I had to fix (e.g the dimensionalities of the matrices).</description>
    </item>
    
    <item>
      <title>New paper: Learning Energy Conserving Dynamics Efficiently with Hamiltonian Gaussian Processess</title>
      <link>/posts/paper2/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/paper2/</guid>
      <description>I spent the summer of 2022 visiting Markus Heinonen at Aalto University in Finland. Together we worked on energy conserving GP models, and I am happy to say our paper on the work was recently accepted to TMLR. Check out the paper, the code, or some visualizations.
  We place a GP prior over the Hamiltonian and, using a set of inducing points, map function samples through Hamilton&amp;rsquo;s equations to obtain system derivative samples, to which we apply ODE solver to obtain sample trajectories.</description>
    </item>
    
    <item>
      <title>Winners curse in Bayesian optimisation</title>
      <link>/posts/winnerscurse_problem/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/posts/winnerscurse_problem/</guid>
      <description>Problem
Take an objective function ( f(x) ) with GP prior and i.i.d. Gaussian noise with variance ( \sigma^2 ), so ( y_i = f(x_i) + \epsilon_i ) with ( \epsilon_i \sim \mathcal{N}(0, \sigma^2) ). Assume two locations ( x_1, x_2 ) are sufficiently separated that their function values ( f(x_1) ) and ( f(x_2) ) are independent. Compute the posterior ( p(\epsilon_1-\epsilon_2|y_1, y_2) ) and use it to justify that the expected improvement in Bayesian optimisation suffers winners curse for a noisy objective.</description>
    </item>
    
    <item>
      <title>New paper: Learning Nonparametric Volterra Kernels with Gaussian Processes</title>
      <link>/posts/paper1/</link>
      <pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/paper1/</guid>
      <description>Update: This paper was published at NeurIPS 2021 check out the final version or my presentation.
This work, with Mauricio Alvarez and Mike Smith, has been the main focus of the first year of my PhD, check it out on arXiv.
  The sampling process for the model described in the paper, which is a non-parametric, and can represent data generated by non-linear operators.
  There will be a blog post explaining the key ideas coming soon!</description>
    </item>
    
    <item>
      <title>Simulation is easy, probability is hard...</title>
      <link>/posts/another-note/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/another-note/</guid>
      <description>There are many interesting things that can be learned about probabilistic modelling from the world of trading and finance, where, perhaps due to the direct connection to very large sums of money, attitudes to problems are generally very different to those of the typical statistician or ML practitioner. Two books I have enjoyed recently in the area are The Education of a Speculator by Victor Niederhoffer and Fooled by Randomness by Nassim Nicholas Taleb.</description>
    </item>
    
    <item>
      <title>Talk at 3rd Sheffield Workshop on Structural Dynamics</title>
      <link>/posts/talk1/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/talk1/</guid>
      <description>Near the end of last year I had the privilege of being invited at the 3rd Sheffield Workshop on Structural Dynamics. I spoke for 15 minutes about some work I have been doing on the application of Gaussian processes to the non-parametric learning of Volterra kernel functions. The event as a whole was super interesting, and I would fully recommend the other talks.
You can find my talk here.</description>
    </item>
    
    <item>
      <title>Podcasts about ML</title>
      <link>/posts/podcasts/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/podcasts/</guid>
      <description>There are a lot of podcasts out there, and this includes loads that are related to machine learning in some way. Over the summer I&amp;rsquo;ve worked my way through a fair few of them, so I thought I&amp;rsquo;d compile a list of my favorites. There are a couple of lists like this out there already, but most of them are out of date, either they include podcasts that are now inactive, or leave out ones that have started recently.</description>
    </item>
    
    <item>
      <title>Lagrangians with PyTorch Part #1</title>
      <link>/posts/l1/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/l1/</guid>
      <description>(Note: This series of posts is closely related to and inspired by this paper from Miles Cranmer, Sam Greydanus, Stephan Hoyer and others. To accompany the paper they also wrote a brilliant blog post about the work which I would encourage you to read)
I really enjoy Lagrangian mechanics, in fact, I would go so far as to say that studying it was one of the best parts of my physics degree.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
